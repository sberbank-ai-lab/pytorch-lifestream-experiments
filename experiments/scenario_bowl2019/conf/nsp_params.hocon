{
  data_module: {
    type: map
    setup: {
      col_id: installation_id
      dataset_files: {
        data_path: "data/train_trx.parquet"
      }
      split_by: files
      valid_size: 0.05
      valid_split_seed: 42
    }
    train: {
      min_seq_len: 25
      split_strategy: {
        split_strategy: "SampleSlices"
        split_count: 4
        cnt_min: 200
        cnt_max: 500
      }
      augmentations: [
        [DropoutTrx, {trx_dropout: 0.01}]
      ]
      num_workers: 8
      batch_size: 256
    }
    valid: {
      split_strategy: {
        split_strategy: SampleSlices
        split_count: 4
        cnt_min: 200
        cnt_max: 500
      }
      augmentations: []
      num_workers: 16
      batch_size: 1024
    }
  }
  logger_name: nsp_model

  seed_everything: 42
  trainer: {
    gpus: 1
    auto_select_gpus: false

    max_epochs: 100

    deterministic: True
  }

  params: {
    data_module_class: dltranz.data_load.data_module.nsp_data_module.NspDataModuleTrain
    pl_module_class: dltranz.lightning_modules.sop_nsp_module.SopNspModule

    encoder_type: rnn,
    trx_encoder: {
      norm_embeddings: false
      embeddings_noise: 0.003
      embeddings: {
        event_id: {in: 500, out: 16}
        event_code: {in: 50, out: 8}
        event_type: {in: 6, out: 8}
        title: {in: 50, out: 8}
        world: {in: 6, out: 8}
        correct: {in: 4, out: 2}
      }
      numeric_values: {}
    }

    rnn: {
      type: gru
      hidden_size: 100
      bidir: false
      trainable_starter: static
    }

    head: {
      hidden_size: 512
      drop_p: 0.5
      pred_all_states: false
    }

    lr_scheduler: {
      ReduceLROnPlateau: true
      patience: 15
    }

    train: {
      lr: 0.001
      weight_decay: 0.0
      use_best_epoch: true
    }
  }

  model_path: "models/nsp_model.p"

  include "dataset_inference_file.hocon"

  output: {
    path: data/nsp_embeddings,
    format: pickle,
  }
}
